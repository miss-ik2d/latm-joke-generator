{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e0e32eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff428b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'h', 'a', 't', ' ', 'd', 'i', 'd', ' ', 't', 'h', 'e', ' ', 'b', 'a', 'r', 't', 'e', 'n', 'd', 'e', 'r', ' ', 's', 'a', 'y', ' ', 't', 'o', ' ', 't', 'h', 'e', ' ', 'j', 'u', 'm', 'p', 'e', 'r', ' ', 'c', 'a', 'b', 'l', 'e', 's', '?', ' ', 'Y', 'o', 'u', ' ', 'b', 'e', 't', 't', 'e', 'r', ' ', 'n', 'o', 't', ' ', 't', 'r', 'y', ' ', 't', 'o', ' ', 's', 't', 'a', 'r', 't', ' ', 'a', 'n', 'y', 't', 'h', 'i', 'n', 'g', '.', '\\n', 'D', 'o', 'n', \"'\", 't', ' ', 'y', 'o', 'u', ' ', 'h', 'a', 't']\n"
     ]
    }
   ],
   "source": [
    "# Get jokes as a numpy array\n",
    "joke_data = pd.read_csv(\"https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv\")['Joke']\n",
    "joke_data = joke_data.to_numpy()\n",
    "\n",
    "# Create a new array of jokes, with a '\\n' interspersed between each joke\n",
    "jokes = []\n",
    "for joke in joke_data:\n",
    "    for ch in joke:\n",
    "        jokes.append(ch)\n",
    "    jokes.append('\\n')\n",
    "    \n",
    "print(jokes[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33e80cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4105d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(arr, batch_size, seq_length):\n",
    "    ''' Function for generating batches (of inputs and outputs)\n",
    "    '''\n",
    "    \n",
    "    chars_per_batch = batch_size * seq_length\n",
    "    n_batches = arr.size//chars_per_batch\n",
    "    \n",
    "    # Cuts off additional numbers from the end of the array and reshape into batch-size rows\n",
    "    arr = arr[:n_batches * chars_per_batch]\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # Iterates through and gets minibatches (targets and features)\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        x = arr[:, n:n+seq_length] # Features\n",
    "        y = np.zeros_like(x) # Targets\n",
    "        \n",
    "        # Y should be x shifted by one\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50d7c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for one_hot encoding numpy arrays using integer-encoded characters\n",
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    arr = arr.astype('int')\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2a737e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class lstm(nn.Module):\n",
    "    def __init__(self, chars, batch_size=10, seq_length=10, hidden_size=256, n_layers=2, batch_first=True, dropout_p=0.3, lr=0.001):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.chars = chars\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.dropout_p = dropout_p\n",
    "        self.lr = lr\n",
    "        self.n_chars = len(chars)\n",
    "        \n",
    "        '''\n",
    "            Architecture: LSTM layers -> dropout layer -> fully connected layer\n",
    "            \n",
    "            chars = set of chars\n",
    "            hidden_size = the number of features in the hidden state of LSTMs\n",
    "            \n",
    "            n_layers = the number of LSTM layers we want to use\n",
    "            batch_first = whether the batch is so it outputs in this order (batch, seq, feature)\n",
    "        '''\n",
    "        \n",
    "        # Dictionaries for character conversion - we need to load these in our checkpoint in order to convert the output.\n",
    "        self.int2char = dict(enumerate(chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # Layers\n",
    "        self.lstm = nn.LSTM(self.n_chars, hidden_size, n_layers, dropout=dropout_p, batch_first=batch_first)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc = nn.Linear(hidden_size, self.n_chars)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.dropout(x)\n",
    "        x = x.contiguous().reshape(-1, self.hidden_size)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, hidden\n",
    "        \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (device == 'cuda'):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_size).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_size).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_size).zero_())\n",
    "            \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bfa8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "def train_model(data, chars, net, batch_size=10, seq_length=10,\n",
    "                epochs=20, val_frac=0.9, lr=0.001, clip=5):\n",
    "    \n",
    "    ''' Function for training your model.\n",
    "    \n",
    "        data: the array of jokes (text), encoded into integers\n",
    "        net: the network that will be trained\n",
    "        batch_size: the number of sequences per batch\n",
    "        seq_length: number of charaters per sequence (should be shorter than the average joke)\n",
    "        epochs: the number of epochs\n",
    "        val_frac: the percentage of data that will be used in the training set (the remaining data\n",
    "            will be used for testing)\n",
    "        lr: learning_rate\n",
    "        clip: gradient clipping\n",
    "    '''\n",
    "    \n",
    "    encoded = np.array([net.char2int[ch] for ch in data])\n",
    "    \n",
    "    # Split data into training and testing sets\n",
    "    val_split = int(len(encoded) * val_frac)\n",
    "    train_data, val_data = encoded[:val_split], encoded[val_split:]\n",
    "    \n",
    "    # Defining criterion and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr)\n",
    "    \n",
    "    # Set up the network\n",
    "    net.train()\n",
    "    net = net.to(device)\n",
    "    \n",
    "    best_state_dict = net.state_dict()\n",
    "    counter = 0 # Counts steps\n",
    "\n",
    "    print(\"Time for training!\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        hidden = net.init_hidden_state(batch_size)\n",
    "        val_losses_total = [] # val losses for an entire round of validation (every batch)\n",
    "        training_losses = [] # val losses for an entire round of training\n",
    "        \n",
    "        for x, y in get_batch(train_data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            # Encode and prepare data\n",
    "            x = one_hot_encode(x, net.n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            x, y = x.to(device), y.to(device)\n",
    "                \n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            \n",
    "            # Forward Pass\n",
    "            net.zero_grad()\n",
    "            predictions, hidden = net(x, hidden)\n",
    "            \n",
    "            loss = criterion(predictions, y.reshape(batch_size * seq_length).long())\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            training_losses.append(loss.detach().cpu())\n",
    "    \n",
    "            # Validation Pass - Calculate\n",
    "            if counter % 200 == 0:\n",
    "                val_h = net.init_hidden_state(batch_size)\n",
    "                net.eval()\n",
    "                val_losses = []\n",
    "                \n",
    "                for x, y in get_batch(val_data, batch_size, seq_length):\n",
    "                    x = one_hot_encode(x, net.n_chars)\n",
    "\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    x, y = x.to(device), y.to(device)\n",
    "\n",
    "                    val_h = tuple([each.data for each in hidden])\n",
    "\n",
    "                    val_pred, val_h = net(x, val_h)\n",
    "                    val_loss = criterion(val_pred, y.reshape(batch_size * seq_length).long())\n",
    "                    val_losses.append(val_loss.detach().cpu())\n",
    "\n",
    "                net.train()\n",
    "                \n",
    "                val_loss_mean = np.mean(val_losses)\n",
    "                # If the validation loss is the smallest, update the best state dict for the checkpoint\n",
    "                if val_losses_total:\n",
    "                    if (val_loss_mean < min(val_losses_total)):\n",
    "                        best_state_dict = net.state_dict()\n",
    "                \n",
    "                val_losses_total.append(val_loss_mean)\n",
    "\n",
    "                print(f\"Epoch: {epoch+1}/{epochs}...\",\n",
    "                            f\"Step: {counter}...\",\n",
    "                            f\"Training Loss: {np.mean(training_losses):.4f}...\",\n",
    "                            f\"Validation Loss: {val_loss_mean:.4f}\")\n",
    "                \n",
    "    print(f\"Done! Lowest val loss: {min(val_losses_total)}\")\n",
    "    \n",
    "    return best_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "455a0d08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for training!\n",
      "Epoch: 1/10... Step: 50... Training Loss: 3.4361... Validation Loss: 3.2140\n",
      "Epoch: 1/10... Step: 100... Training Loss: 3.3379... Validation Loss: 3.2063\n",
      "Epoch: 1/10... Step: 150... Training Loss: 3.3143... Validation Loss: 3.2105\n",
      "Epoch: 1/10... Step: 200... Training Loss: 3.2921... Validation Loss: 3.1842\n",
      "Epoch: 1/10... Step: 250... Training Loss: 3.2550... Validation Loss: 2.9667\n",
      "Epoch: 1/10... Step: 300... Training Loss: 3.1885... Validation Loss: 2.7903\n",
      "Epoch: 1/10... Step: 350... Training Loss: 3.1163... Validation Loss: 2.7145\n",
      "Epoch: 2/10... Step: 400... Training Loss: 2.5621... Validation Loss: 2.6349\n",
      "Epoch: 2/10... Step: 450... Training Loss: 2.5580... Validation Loss: 2.5561\n",
      "Epoch: 2/10... Step: 500... Training Loss: 2.5178... Validation Loss: 2.5216\n",
      "Epoch: 2/10... Step: 550... Training Loss: 2.5047... Validation Loss: 2.5039\n",
      "Epoch: 2/10... Step: 600... Training Loss: 2.4781... Validation Loss: 2.4599\n",
      "Epoch: 2/10... Step: 650... Training Loss: 2.4531... Validation Loss: 2.4818\n",
      "Epoch: 2/10... Step: 700... Training Loss: 2.4345... Validation Loss: 2.4339\n",
      "Epoch: 2/10... Step: 750... Training Loss: 2.4157... Validation Loss: 2.4158\n",
      "Epoch: 3/10... Step: 800... Training Loss: 2.2485... Validation Loss: 2.4173\n",
      "Epoch: 3/10... Step: 850... Training Loss: 2.2613... Validation Loss: 2.3545\n",
      "Epoch: 3/10... Step: 900... Training Loss: 2.2457... Validation Loss: 2.4096\n",
      "Epoch: 3/10... Step: 950... Training Loss: 2.2468... Validation Loss: 2.3469\n",
      "Epoch: 3/10... Step: 1000... Training Loss: 2.2344... Validation Loss: 2.3386\n",
      "Epoch: 3/10... Step: 1050... Training Loss: 2.2165... Validation Loss: 2.3716\n",
      "Epoch: 3/10... Step: 1100... Training Loss: 2.2079... Validation Loss: 2.3122\n",
      "Epoch: 3/10... Step: 1150... Training Loss: 2.1977... Validation Loss: 2.3252\n",
      "Epoch: 4/10... Step: 1200... Training Loss: 2.0920... Validation Loss: 2.2993\n",
      "Epoch: 4/10... Step: 1250... Training Loss: 2.1018... Validation Loss: 2.2975\n",
      "Epoch: 4/10... Step: 1300... Training Loss: 2.0969... Validation Loss: 2.2698\n",
      "Epoch: 4/10... Step: 1350... Training Loss: 2.0951... Validation Loss: 2.2504\n",
      "Epoch: 4/10... Step: 1400... Training Loss: 2.0883... Validation Loss: 2.2743\n",
      "Epoch: 4/10... Step: 1450... Training Loss: 2.0780... Validation Loss: 2.2397\n",
      "Epoch: 4/10... Step: 1500... Training Loss: 2.0670... Validation Loss: 2.2744\n",
      "Epoch: 4/10... Step: 1550... Training Loss: 2.0588... Validation Loss: 2.2512\n",
      "Epoch: 5/10... Step: 1600... Training Loss: 1.9919... Validation Loss: 2.2889\n",
      "Epoch: 5/10... Step: 1650... Training Loss: 1.9861... Validation Loss: 2.2239\n",
      "Epoch: 5/10... Step: 1700... Training Loss: 1.9861... Validation Loss: 2.2682\n",
      "Epoch: 5/10... Step: 1750... Training Loss: 1.9777... Validation Loss: 2.2352\n",
      "Epoch: 5/10... Step: 1800... Training Loss: 1.9696... Validation Loss: 2.2396\n",
      "Epoch: 5/10... Step: 1850... Training Loss: 1.9634... Validation Loss: 2.1978\n",
      "Epoch: 5/10... Step: 1900... Training Loss: 1.9567... Validation Loss: 2.2343\n",
      "Epoch: 6/10... Step: 1950... Training Loss: 1.8586... Validation Loss: 2.2357\n",
      "Epoch: 6/10... Step: 2000... Training Loss: 1.8992... Validation Loss: 2.2088\n",
      "Epoch: 6/10... Step: 2050... Training Loss: 1.8981... Validation Loss: 2.2083\n",
      "Epoch: 6/10... Step: 2100... Training Loss: 1.9031... Validation Loss: 2.2076\n",
      "Epoch: 6/10... Step: 2150... Training Loss: 1.8951... Validation Loss: 2.1972\n",
      "Epoch: 6/10... Step: 2200... Training Loss: 1.8820... Validation Loss: 2.2624\n",
      "Epoch: 6/10... Step: 2250... Training Loss: 1.8786... Validation Loss: 2.2109\n",
      "Epoch: 6/10... Step: 2300... Training Loss: 1.8732... Validation Loss: 2.1795\n",
      "Epoch: 7/10... Step: 2350... Training Loss: 1.8145... Validation Loss: 2.2403\n",
      "Epoch: 7/10... Step: 2400... Training Loss: 1.8168... Validation Loss: 2.2181\n",
      "Epoch: 7/10... Step: 2450... Training Loss: 1.8205... Validation Loss: 2.1590\n",
      "Epoch: 7/10... Step: 2500... Training Loss: 1.8240... Validation Loss: 2.1616\n",
      "Epoch: 7/10... Step: 2550... Training Loss: 1.8206... Validation Loss: 2.1846\n",
      "Epoch: 7/10... Step: 2600... Training Loss: 1.8061... Validation Loss: 2.1764\n",
      "Epoch: 7/10... Step: 2650... Training Loss: 1.8017... Validation Loss: 2.2083\n",
      "Epoch: 7/10... Step: 2700... Training Loss: 1.7994... Validation Loss: 2.2093\n",
      "Epoch: 8/10... Step: 2750... Training Loss: 1.7409... Validation Loss: 2.1780\n",
      "Epoch: 8/10... Step: 2800... Training Loss: 1.7534... Validation Loss: 2.1918\n",
      "Epoch: 8/10... Step: 2850... Training Loss: 1.7617... Validation Loss: 2.2197\n",
      "Epoch: 8/10... Step: 2900... Training Loss: 1.7613... Validation Loss: 2.2387\n",
      "Epoch: 8/10... Step: 2950... Training Loss: 1.7573... Validation Loss: 2.1881\n",
      "Epoch: 8/10... Step: 3000... Training Loss: 1.7483... Validation Loss: 2.1968\n",
      "Epoch: 8/10... Step: 3050... Training Loss: 1.7440... Validation Loss: 2.1904\n",
      "Epoch: 8/10... Step: 3100... Training Loss: 1.7393... Validation Loss: 2.2158\n",
      "Epoch: 9/10... Step: 3150... Training Loss: 1.7054... Validation Loss: 2.1972\n",
      "Epoch: 9/10... Step: 3200... Training Loss: 1.7051... Validation Loss: 2.2129\n",
      "Epoch: 9/10... Step: 3250... Training Loss: 1.7110... Validation Loss: 2.1880\n",
      "Epoch: 9/10... Step: 3300... Training Loss: 1.7097... Validation Loss: 2.2425\n",
      "Epoch: 9/10... Step: 3350... Training Loss: 1.7033... Validation Loss: 2.2090\n",
      "Epoch: 9/10... Step: 3400... Training Loss: 1.6988... Validation Loss: 2.2048\n",
      "Epoch: 9/10... Step: 3450... Training Loss: 1.6933... Validation Loss: 2.1772\n",
      "Epoch: 10/10... Step: 3500... Training Loss: 1.6361... Validation Loss: 2.3168\n",
      "Epoch: 10/10... Step: 3550... Training Loss: 1.6549... Validation Loss: 2.2247\n",
      "Epoch: 10/10... Step: 3600... Training Loss: 1.6582... Validation Loss: 2.2319\n",
      "Epoch: 10/10... Step: 3650... Training Loss: 1.6619... Validation Loss: 2.2076\n",
      "Epoch: 10/10... Step: 3700... Training Loss: 1.6594... Validation Loss: 2.2196\n",
      "Epoch: 10/10... Step: 3750... Training Loss: 1.6505... Validation Loss: 2.2436\n",
      "Epoch: 10/10... Step: 3800... Training Loss: 1.6479... Validation Loss: 2.1794\n",
      "Epoch: 10/10... Step: 3850... Training Loss: 1.6462... Validation Loss: 2.2025\n",
      "Done! Lowest val loss: 2.179442882537842\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 30\n",
    "seq_length = 10\n",
    "\n",
    "hidden_size = 512\n",
    "n_layers = 3\n",
    "\n",
    "dropout_p = 0.5\n",
    "epochs = 10\n",
    "val_frac = 0.9\n",
    "\n",
    "# Get the chars\n",
    "chars = set([])\n",
    "for joke in jokes:\n",
    "    for char in joke:\n",
    "        chars.add(char)\n",
    "chars = tuple(chars)\n",
    "\n",
    "# Checkpoint\n",
    "\n",
    "net = lstm(chars, batch_size=batch_size, seq_length=seq_length, hidden_size=hidden_size,\n",
    "              dropout_p=dropout_p, n_layers=n_layers)\n",
    "state_dict = train_model(jokes, chars, net, epochs=epochs, batch_size=batch_size,\n",
    "                         seq_length=seq_length, val_frac=val_frac)\n",
    "\n",
    "checkpoint = {'hidden_size': net.hidden_size,\n",
    "              'n_layers': net.n_layers,\n",
    "              'tokens': net.chars,\n",
    "              'state_dict': state_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a13bd451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'lstm_100_epoch.net'\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4638b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        # One-hot encode data\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        h = tuple([each.data for each in h])\n",
    "        \n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        \n",
    "        if(device == 'cuda'):\n",
    "            p = p.to('cpu')\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "250d5e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_joke(net, size, prime='The', top_k=20):\n",
    "        \n",
    "    net = net.to(device)\n",
    "    net.eval()\n",
    "\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden_state(1)\n",
    "    \n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "        \n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        \n",
    "        if (char == '\\n'):\n",
    "            break\n",
    "        \n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24696cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three men walked into a bar. As the first day this is a single thing a barber, the bar thinks he has to came home the bar. \"What is the munches that?' The bartender said, \"Well how do you can't be changed that sir?\" \"Well, you chouse.\"\n"
     ]
    }
   ],
   "source": [
    "# Load model and predict\n",
    "with open('lstm_100_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = lstm(checkpoint['tokens'], hidden_size=checkpoint['hidden_size'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "prime = \"Three men walked into a bar.\"\n",
    "print(finish_joke(loaded, 500, prime=prime, top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce550fd",
   "metadata": {},
   "outputs": [],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
